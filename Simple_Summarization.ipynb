{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extractive Summarization\n",
    "\n",
    "Here we try to use a very simple algorithm to create a text summary. Text Summarization is of two types: **Extractive Summarization** and **Abstractive Summarization**. We will be extracting text summnary using the **Extractive Summarization Technique**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import Spacy \n",
    "import spacy # for filtering and text processing\n",
    "from collections import Counter # to monitor the word count\n",
    "from string import punctuation  # to create the list of punctuations that need to be removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Large english model\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are interested in extracting the keywords from the long text, meaning the sentences. The following steps need to be followed in lorder to extract keywords.\n",
    "\n",
    "- Tokenize the text\n",
    "- Remove the stopwords and punctuations\n",
    "- Filter the words with necessary POS tags\n",
    "- Selectively lower casing words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the list of stop words\n",
    "from stop_words import get_stop_words\n",
    "STOPWORDS = get_stop_words('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "174"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(STOPWORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we are removing 32 punctuations and 174 stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function to extract keywords\n",
    "def extract_words(text):\n",
    "    result = []\n",
    "    # creating an object of language model\n",
    "    doc = nlp(text)\n",
    "    for d in doc:\n",
    "        if(d.text in STOPWORDS or d.text in punctuation):\n",
    "            continue\n",
    "        if(d.pos_ in ['PROPN', 'ADJ', 'NOUN']):\n",
    "            result.append(d.text)\n",
    "                \n",
    "    return result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function that takes a list of keywords and scores the words according to their freq.\n",
    "def norm_scores(list_words):\n",
    "    # here list_word is the list of lists of words, constructed from the extract_words func.\n",
    "    lw = [item for sublist in list_words for item in sublist]\n",
    "    word_count = Counter(lw)\n",
    "    max_freq = Counter(lw).most_common(1)[0][1]\n",
    "    for k in word_count:\n",
    "        word_count[k] /= max_freq\n",
    "    # returns a list keywords with their scores depending upon frequency\n",
    "    return word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function that will calculate the importance of a sentence based on number of \n",
    "# keywords and relative importance of those keywords as reflected by their frequency in \n",
    "# the text\n",
    "def sent_score(text,word_count):\n",
    "    list_score = []\n",
    "    for sent in nlp(text).sents:\n",
    "        sent_score = extract_words(str(sent))\n",
    "        if len(sent_score) == 0:\n",
    "            list_score.append(0)\n",
    "            continue\n",
    "        else:\n",
    "            s = 0\n",
    "            for w in sent_score:\n",
    "                s += word_count[w]\n",
    "            list_score.append(s)\n",
    "    return list_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [str(sent) for sent in nlp(text).sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I’ve always been a bargain shopper.',\n",
       " 'When I moved to New York in 2000 I discovered H&M.',\n",
       " 'At the time, fast fashion didn’t mean sweatshop labor and climate damage —',\n",
       " 'it meant that I could find a brand-new sensible office dress for $14.99 and still have enough money to pay for groceries.',\n",
       " 'I thought my penchant for cheap clothing was temporary, that sometime in my 30s, after a decade of working in the corporate world, a switch would flip and suddenly the clothing I saw in fashion magazines would become available to me like a birthright.',\n",
       " 'It hasn’t happened yet.']"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_words = [extract_words(sent) for sent in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['bargain', 'shopper'],\n",
       " ['New', 'York', 'H&M.'],\n",
       " ['time', 'fast', 'fashion', 'sweatshop', 'labor', 'climate', 'damage'],\n",
       " ['brand',\n",
       "  'new',\n",
       "  'sensible',\n",
       "  'office',\n",
       "  'dress',\n",
       "  'enough',\n",
       "  'money',\n",
       "  'groceries'],\n",
       " ['penchant',\n",
       "  'cheap',\n",
       "  'clothing',\n",
       "  'temporary',\n",
       "  '30s',\n",
       "  'decade',\n",
       "  'corporate',\n",
       "  'world',\n",
       "  'switch',\n",
       "  'clothing',\n",
       "  'fashion',\n",
       "  'magazines',\n",
       "  'available',\n",
       "  'birthright'],\n",
       " []]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count = norm_scores(list_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'bargain': 0.5,\n",
       "         'shopper': 0.5,\n",
       "         'New': 0.5,\n",
       "         'York': 0.5,\n",
       "         'H&M.': 0.5,\n",
       "         'time': 0.5,\n",
       "         'fast': 0.5,\n",
       "         'fashion': 1.0,\n",
       "         'sweatshop': 0.5,\n",
       "         'labor': 0.5,\n",
       "         'climate': 0.5,\n",
       "         'damage': 0.5,\n",
       "         'brand': 0.5,\n",
       "         'new': 0.5,\n",
       "         'sensible': 0.5,\n",
       "         'office': 0.5,\n",
       "         'dress': 0.5,\n",
       "         'enough': 0.5,\n",
       "         'money': 0.5,\n",
       "         'groceries': 0.5,\n",
       "         'penchant': 0.5,\n",
       "         'cheap': 0.5,\n",
       "         'clothing': 1.0,\n",
       "         'temporary': 0.5,\n",
       "         '30s': 0.5,\n",
       "         'decade': 0.5,\n",
       "         'corporate': 0.5,\n",
       "         'world': 0.5,\n",
       "         'switch': 0.5,\n",
       "         'magazines': 0.5,\n",
       "         'available': 0.5,\n",
       "         'birthright': 0.5})"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, 1.5, 4.0, 4.0, 8.5, 0]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_score(text,word_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** We quickly that the sentence score not only depends on the length of the sentence but also the number of keywords present in the sentence which is the basic premise of associating importance to sentences. So, we could arrange the sentences of the text according to its importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = sent_score(text,word_count)\n",
    "sent_importance = dict()\n",
    "\n",
    "for s,sent in zip(scores,nlp(text).sents):\n",
    "    sent_importance[str(sent)] = s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'I’ve always been a bargain shopper.': 1.0,\n",
       " 'When I moved to New York in 2000 I discovered H&M.': 1.5,\n",
       " 'At the time, fast fashion didn’t mean sweatshop labor and climate damage —': 4.0,\n",
       " 'it meant that I could find a brand-new sensible office dress for $14.99 and still have enough money to pay for groceries.': 4.0,\n",
       " 'I thought my penchant for cheap clothing was temporary, that sometime in my 30s, after a decade of working in the corporate world, a switch would flip and suddenly the clothing I saw in fashion magazines would become available to me like a birthright.': 8.5,\n",
       " 'It hasn’t happened yet.': 0}"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_sents = {k: v for k, v in sorted(sent_importance.items(), key=lambda item: item[1], reverse=True)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importance Rank:1\n",
      "I thought my penchant for cheap clothing was temporary, that sometime in my 30s, after a decade of working in the corporate world, a switch would flip and suddenly the clothing I saw in fashion magazines would become available to me like a birthright.\n",
      "Importance Rank:2\n",
      "At the time, fast fashion didn’t mean sweatshop labor and climate damage —\n",
      "Importance Rank:3\n",
      "it meant that I could find a brand-new sensible office dress for $14.99 and still have enough money to pay for groceries.\n",
      "Importance Rank:4\n",
      "When I moved to New York in 2000 I discovered H&M.\n",
      "Importance Rank:5\n",
      "I’ve always been a bargain shopper.\n",
      "Importance Rank:6\n",
      "It hasn’t happened yet.\n"
     ]
    }
   ],
   "source": [
    "for i,k in enumerate(sorted_sents.keys()):\n",
    "    print(\"Importance Rank:{}\".format(i+1))\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shortcomings:\n",
    "\n",
    "Although, this method is simple to execute and intuitively associates importance to a sentence with more keywords that occur in a document more frequently, it has the following shorcomings:\n",
    "\n",
    "- By combining the sentences that are important as portrayed by their individual scores we find that the resulting text may lack coherance, especially when the text on which the scoring is done is bigger than a paragraph.\n",
    "- Certain non-textual keywords such as dates and numeric quantities .eg. prices, are not included. Depending upon our use case they might be important.\n",
    "- All the tokens are taken as unigrams, in future n-grams must be included. So it should be key phrases extraction, instead of keyword extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
